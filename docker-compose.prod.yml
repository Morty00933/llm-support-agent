# LLM Support Agent - Production Environment
#
# Usage:
#   make prod                   # Start production
#   make prod-monitoring        # Start with monitoring
#
# Differences from dev:
# - Uses production Dockerfile stages
# - No volume mounts (code baked into images)
# - Security hardened
# - Resource limits
# - Production-grade restart policies

services:
  # ============================================================
  # DATABASE
  # ============================================================

  postgres:
    image: pgvector/pgvector:pg16
    container_name: llm-support-db-prod
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
      # Performance tuning for production
      POSTGRES_SHARED_BUFFERS: 512MB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 2GB
      POSTGRES_MAX_CONNECTIONS: 200
      POSTGRES_WORK_MEM: 16MB
      POSTGRES_MAINTENANCE_WORK_MEM: 128MB
    ports:
      - "127.0.0.1:${DB_PORT:-5432}:5432"  # Bind to localhost only
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    restart: always
    networks:
      - llm-support-network

  # ============================================================
  # CACHE
  # ============================================================

  redis:
    image: redis:7-alpine
    container_name: llm-support-redis-prod
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD}
      --appendonly yes
      --appendfilename "appendonly.aof"
      --maxmemory 1gb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
    ports:
      - "127.0.0.1:${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    restart: always
    networks:
      - llm-support-network

  # ============================================================
  # AI/LLM
  # ============================================================

  ollama:
    image: ollama/ollama:latest
    container_name: llm-support-ollama-prod
    ports:
      - "127.0.0.1:11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=2
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
    restart: always
    networks:
      - llm-support-network

  # ============================================================
  # BACKEND API
  # ============================================================

  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
      target: production
    image: llm-support-backend:latest
    container_name: llm-support-api-prod
    ports:
      - "127.0.0.1:${PORT:-8000}:8000"
    environment:
      # App config
      - ENV=prod
      - DEBUG=false
      - APP_NAME=${APP_NAME}
      - HOST=0.0.0.0
      - PORT=8000
      - WORKERS=${WORKERS:-4}

      # Database
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_NAME=${DB_NAME}
      - DB_POOL_SIZE=20
      - DB_MAX_OVERFLOW=40

      # Redis
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - REDIS_DB=0

      # Ollama
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL_CHAT=${OLLAMA_MODEL_CHAT}
      - OLLAMA_MODEL_EMBED=${OLLAMA_MODEL_EMBED}

      # Security
      - JWT_SECRET=${JWT_SECRET}
      - JWT_ALG=HS256
      - JWT_EXPIRE_MIN=60

      # CORS (production domains)
      - CORS_ORIGINS=${CORS_ORIGINS}

      # Logging
      - LOG_LEVEL=INFO
      - LOG_FORMAT=json
      - SENTRY_DSN=${SENTRY_DSN:-}

      # Features
      - PROMETHEUS_ENABLED=true

      # Celery
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - CELERY_RESULT_BACKEND=redis://:${REDIS_PASSWORD}@redis:6379/0
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
      replicas: 2  # For load balancing with nginx
    restart: always
    networks:
      - llm-support-network

  # ============================================================
  # FRONTEND
  # ============================================================

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: production
    image: llm-support-frontend:latest
    container_name: llm-support-frontend-prod
    ports:
      - "80:3000"
      - "443:3000"  # If using SSL
    depends_on:
      backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M
    restart: always
    networks:
      - llm-support-network

  # ============================================================
  # CELERY WORKER
  # ============================================================

  celery:
    build:
      context: .
      dockerfile: Dockerfile.backend
      target: production
    image: llm-support-backend:latest
    container_name: llm-support-celery-prod
    command: celery -A src.core.celery_app worker --loglevel=info --concurrency=4 --max-tasks-per-child=1000
    environment:
      - ENV=prod
      - DB_HOST=postgres
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_NAME=${DB_NAME}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL_CHAT=${OLLAMA_MODEL_CHAT}
      - OLLAMA_MODEL_EMBED=${OLLAMA_MODEL_EMBED}
      - JWT_SECRET=${JWT_SECRET}
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - CELERY_RESULT_BACKEND=redis://:${REDIS_PASSWORD}@redis:6379/0
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
      replicas: 2
    restart: always
    networks:
      - llm-support-network

# ============================================================
# VOLUMES
# ============================================================

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  ollama_data:
    driver: local

# ============================================================
# NETWORKS
# ============================================================

networks:
  llm-support-network:
    driver: bridge
    name: llm-support-network-prod
