# LLM Support Agent — README

## Обзор

**LLM Support Agent** — это полностью локализуемая система поддержки, объединяющая тикетинг, базу знаний, локальные модели LLM (через Ollama), асинхронные интеграции (Jira/Zendesk), мультитенантность и удобный веб-интерфейс.
Система создана для компаний, которые хотят:

* использовать собственные данные без передачи их третьим сторонам;
* расширить службу поддержки интеллектуальными автозапросами;
* централизовать тикеты, диалоги и knowledge-base контент;
* интегрироваться с существующими helpdesk-платформами.

Проект разворачивается через Docker, полностью автономен и может работать даже без доступа в Интернет.

---

# Содержание

1. [Архитектура](#архитектура)
2. [Технологии](#технологии)
3. [Быстрый старт](#быстрый-старт)
4. [Структура репозитория](#структура-репозитория)
5. [Бэкенд](#бэкенд)
6. [Фронтенд](#фронтенд)
7. [База знаний](#база-знаний)
8. [Работа агента](#работа-агента)
9. [Интеграции Jira/Zendesk](#интеграции-jira-zendesk)
10. [Мультитенантность](#мультитенантность)
11. [Миграции БД и тестирование](#миграции-бд-и-тестирование)
12. [Охрана, логирование и метрики](#охрана-логирование-и-метрики)
13. [Настройка окружения](#настройка-окружения)
14. [Планы развития](#планы-развития)

---

# Архитектура

## Компоненты

| Компонент                 | Описание                                                                                         |
| ------------------------- | ------------------------------------------------------------------------------------------------ |
| **UI (React + Nginx)**    | Веб-панель для работы с тикетами, базой знаний, агентом и настройками.                           |
| **API (FastAPI)**         | REST-сервер, маршруты `/v1/kb`, `/v1/tickets`, `/v1/support`, `/v1/auth`, `/metrics`, `/health`. |
| **LLM (Ollama)**          | Модели для генерации ответов и для эмбеддингов. Запускается локально.                            |
| **PostgreSQL + pgvector** | Хранилище тикетов, сообщений, KB-чанков, внешних ссылок, синхронизаций.                          |
| **Redis**                 | Брокер задач Celery.                                                                             |
| **Celery worker**         | Асинхронные процессоры для Jira/Zendesk синхронизаций и фоновых операций.                        |

В результате получается распределённая система, где LLM-ответы формируются синхронно, а интеграции с внешними системами — асинхронно и без блокировки UI.

---

# Технологии

* **Backend:** FastAPI, SQLAlchemy 2.0, Alembic, Celery, Redis, httpx, structlog
* **ML:** Ollama, pgvector
* **Frontend:** React 18, TypeScript, Vite, TailwindCSS, Zustand, React Query
* **Инфра:** Docker, docker-compose, Nginx
* **Наблюдаемость:** Prometheus metrics, Sentry/OTEL (опционально)

---

# Быстрый старт

## Предусловия

* Docker + Docker Compose
* 8GB RAM минимум (лучше 16GB, если используете грандиозные модели)
* Некоторый объём на диске (модели Ollama весят 1–5GB)

## Установка

1. Скопируйте `.env.example` в `.env`:

   ```bash
   cp .env.example .env
   ```

2. Запустите:

   ```bash
   make run
   ```

3. Откройте UI:

   ```
   http://localhost:8080
   ```

4. Войдите:

   * tenant: **1**
   * email: **[user@example.com](mailto:user@example.com)**
   * пароль: **любой** (в DEMO режиме используется упрощённая авторизация)

5. Проверьте работоспособность:

   ```bash
   make smoke
   ```

---

# Структура репозитория

```
llm/
├── src/               # Backend
│   ├── api/           # FastAPI endpoints
│   ├── agent/         # LLM agent logic
│   ├── services/      # KB, embeddings, integrations
│   ├── domain/        # ORM models, repositories
│   ├── core/          # Config, DB, logging, Celery, metrics
│   └── tasks/         # Celery tasks
│
├── ui/                # Frontend (React)
├── migrations/        # Alembic migrations
├── ops/               # Entrypoints
├── scripts/           # smoke-тест
└── docker-compose.yml
```

---

# Бэкенд

## Основной стек

* FastAPI раздаёт эндпоинты под `/v1/**`.
* SQLAlchemy (async) управляет транзакциями через `get_session()`.
* Все операции изолированы по tenant’у.
* Celery выполняет интеграции в фоне.
* Метрики доступны по `/metrics`.

## Основные маршруты

### `/v1/auth/login`

Возвращает JWT. В прод-режиме рекомендуется включить полноценную проверку паролей.

### `/v1/tickets/**`

Управление тикетами:

* создание тикета,
* просмотр истории,
* добавление сообщений,
* получение ответа агента.

### `/v1/kb/**`

Работа с базой знаний:

* загрузка чанков,
* поиск,
* архивирование/удаление,
* переиндексация эмбеддингов.

### `/v1/support/**`

Запросы к LLM:

* простой запрос агента («чат»),
* запрос по тикету с использованием контекста KB,
* автоматическое решение об эскалации.

---

# Фронтенд

Работает через Nginx, проксируя API на `api:8000`.
Функциональные экраны:

* **Tickets** — создание тикетов, просмотр сообщений, запуск агента.
* **Knowledge Base** — загрузка и поиск по KB.
* **Agent Playground** — свободный чат с LLM.
* **Settings** — tenant, URL API, токен.

React Query кеширует данные и повторяет запросы при нестабильной сети.

---

# База знаний

KB — это набор чанков, каждый содержит:

* текст (фрагмент статьи, документа, инструкции),
* хэш chunk_hash (SHA-256),
* эмбеддинг,
* вектор (если pgvector включён),
* метаданные (tags, language, external_id),
* tenant_id.

### Вставка / Upsert

Эндпоинт `/v1/kb/upsert`:

* принимает список чанков;
* каждому тексту считает embedding;
* сохраняет в БД через `INSERT … ON CONFLICT (tenant, source, hash)`.

### Поиск

`/v1/kb/search`:

1. LLM формирует embedding поискового запроса.
2. Если pgvector включён — поиск через `embedding_vector <-> query_vector`.
3. Если pgvector отключён — cosine-similarity в приложении.
4. Возвращает топ-N релевантных фрагментов.

---

# Работа агента

Алгоритм `answer_for_ticket`:

1. Загрузить тикет и историю сообщений.
2. Преобразовать историю в компактный текст.
3. Сформировать поисковый запрос: *заголовок тикета + последнее сообщение*.
4. Найти топ релевантных KB-чанков.
5. Сформировать системный промпт:

   * политику LLM,
   * контекст KB,
   * историю диалога,
   * ограничения (не выдумывать, быть кратким).
6. Отправить в Ollama `/api/chat`.
7. Проанализировать ответ.
8. Принять решение об эскалации:

   * эвристики по ключевым словам,
   * факторы отсутствия контекста.
9. Вернуть JSON-результат: текст ответа, KB-хиты, эскалация (да/нет).

Также есть метод свободного общения `/v1/support/agent/answer`.

---

# Интеграции Jira/Zendesk

Асинхронные цепочки выполняются через Celery:

1. Агент решает эскалировать тикет.
2. API ставит задачу `sync_ticket_task`.
3. Celery worker выполняет:

   * сбор сообщения/контекста,
   * создание Jira issue или Zendesk ticket,
   * запись внешнего ID (`TicketExternalRef`),
   * сохранение логов (`IntegrationSyncLog`).

Доступные переменные:

* `JIRA_ENABLED`, `JIRA_BASE_URL`, `JIRA_EMAIL`, `JIRA_API_TOKEN`
* `ZENDESK_ENABLED`, `ZENDESK_SUBDOMAIN`, `ZENDESK_EMAIL`, `ZENDESK_API_TOKEN`

---

# Мультитенантность

Каждый запрос указывает tenant:

* либо через JWT `tenant: <id>`,
* либо заголовок `X-Tenant-Id`.

Все таблицы содержат `tenant_id`, и все выборки проходят через фильтры.

Это позволяет использовать один экземпляр приложения для нескольких организаций.

---

# Миграции БД и тестирование

## Миграции

* Alembic хранится в каталоге `migrations/`.
* При запуске контейнера `api` автоматически выполняется:

  ```
  alembic upgrade head
  ```

## Тесты

* Unit-тесты (`tests/`) для health и auth.
* Smoke-тест (`scripts/smoke.py`) прогоняет:

  * здоровье системы,
  * логин,
  * загрузку KB,
  * поиск KB.

Для CI рекомендуется:

* mypy
* ruff / black
* pytest
* smoke
* docker build

---

# Охрана, логирование и метрики

## Логирование

Используется `structlog`:

* JSON-формат;
* Timestamps;
* Ошибки с stacktrace.

## Метрики

Доступны по маршруту `/metrics`:

* `http_requests_total{method,path,status}`
* `http_latency_seconds_bucket{method,path}`
* `celery_tasks_total{name,status}`
* `integration_sync_total{system,status}`

Можно собрать дашборд в Grafana.

## Health

* `/health` — общий статус API.
* `/health/deps` — проверка связи с PostgreSQL и Ollama.

---

# Настройка окружения

## Основные переменные `.env`

### LLM

```
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL_CHAT=qwen2.5:3b
OLLAMA_MODEL_EMBED=nomic-embed-text-v1.5
EMBEDDING_DIM=768
```

### База данных

```
DB_HOST=db
DB_PORT=5432
DB_USER=postgres
DB_PASSWORD=postgres
DB_NAME=llm
```

### Redis/Celery

```
REDIS_HOST=redis
CELERY_BROKER_URL=redis://redis:6379/0
```

### JWT

```
JWT_SECRET=super-secret
JWT_AUD=llm
JWT_ISS=llm-agent
JWT_EXPIRE_MIN=43200
```

### CORS / Hosts

```
CORS_ORIGINS=http://localhost:8080
TRUSTED_HOSTS=localhost
```

### Jira / Zendesk (опционально)

```
JIRA_ENABLED=false
JIRA_BASE_URL=
JIRA_EMAIL=
JIRA_API_TOKEN=
```

---

# Планы развития

* полноценная аутентификация (bcrypt + проверки в login эндпоинте);
* конфигурируемый scoring-алгоритм KB;
* улучшенная логика эскалации;
* UI-панель для мониторинга интеграций и состояния Celery;
* пакетное импортирование KB из Confluence/Notion;
* ролевая модель пользователей;
* мультимодельный режим (несколько моделей LLM);
* webhooks для внешних систем.

---

# Лицензия

Проект может быть адаптирован под нужды организации.
Перед коммерческим использованием рекомендуется провести аудит безопасности.

---

# Контакты

Для вопросов по структуре, доработке или развёртыванию можно обращаться к разработчику проекта или команде, которая обслуживает этот репозиторий.

---
